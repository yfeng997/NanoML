{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shakespeare GPT \n",
    "\n",
    "In this notebook we will explore creating a GPT to generate Shakespeare essays. We will start with a simple bigram model, then build up to a full multi-headed GPT model. We will explore a range of techniques and see quantitatively and qualitatively how they affect our training performance. \n",
    "\n",
    "Note the full model in the end should be trained on a GPU to get loss close to 1.4 and reasonable Shakespeare like writings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-09-13 17:13:18--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
      "\n",
      "2024-09-13 17:13:18 (21.2 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('shakespeare.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "char_to_ix = {ch:i for i,ch in enumerate(chars)}\n",
    "ix_to_char = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "def encode(text):\n",
    "    return [char_to_ix[ch] for ch in text]\n",
    "\n",
    "def decode(vec):\n",
    "    return ''.join([ix_to_char[ix] for ix in vec])\n",
    "\n",
    "print(''.join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 14, 43, 44, 53, 56, 43, 1, 61, 43, 1, 54, 56, 53, 41, 43, 43, 42, 1, 39, 52, 63, 1, 44, 59, 56, 58, 46, 43, 56, 6, 1, 46, 43, 39, 56, 1, 51, 43, 1, 57, 54, 43, 39, 49, 8, 0, 0, 13, 50, 50, 10, 0, 31, 54, 43, 39, 49, 6, 1, 57, 54, 43, 39, 49, 8, 0, 0, 18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 37, 53, 59]\n",
      "1115394\n"
     ]
    }
   ],
   "source": [
    "text_vec = encode(text)\n",
    "print(text_vec[:100])\n",
    "print(len(text_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394])\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.tensor(text_vec, dtype=torch.long)\n",
    "print(data.shape)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data))\n",
    "train_data, val_data = data[:n], data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8]) torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "def get_batch(split, batch_size, block_size):\n",
    "    \"\"\"\n",
    "    Each data is multiple examples\n",
    "    \"\"\"\n",
    "    if split == 'train':\n",
    "        data = train_data\n",
    "    else:\n",
    "        # (N x 1)\n",
    "        data = val_data\n",
    "    # B x 1\n",
    "    start = torch.randint(0, len(data) - block_size, (batch_size,))\n",
    "    # [1, 2, 3, 4, 5], block_size = 3\n",
    "    # [[1, 2, 3], [2, 3, 4], [3, 4, 5]]\n",
    "    x = torch.stack([data[s:s+block_size] for s in start])\n",
    "    # [[2, 3, 4], [3, 4, 5], [4, 5, 6]]\n",
    "    y = torch.stack([data[s+1:s+block_size+1] for s in start])\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch('train', 4, 8)\n",
    "# this is 4 * 8 samples \n",
    "print(x.shape, y.shape)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram model\n",
    "\n",
    "In this model, each token carries a fixed probability distribution of its next token. \n",
    "\n",
    "Bigram is the simplest possible language model. Used as a baseline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(5.0496, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# simplest possible language model \n",
    "class BigramLanguageModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_emb = torch.nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, target):\n",
    "        # idx: B x block_size\n",
    "        # B x block_size x vocab_size\n",
    "        # each term in the sequence is mapped to an embedding\n",
    "        # in this case, not an actual embedding, but probabilities for next token\n",
    "        logits = self.token_emb(idx)  \n",
    "        # logits: B x block_size x vocab_size\n",
    "        # target: B x block_size\n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B*T, C)\n",
    "        \n",
    "        if target is not None:\n",
    "            target = target.view(-1)\n",
    "            loss = torch.nn.functional.cross_entropy(logits, target)\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits, loss \n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx: B x T\n",
    "        for _ in range(max_new_tokens):\n",
    "            # T is getting updated\n",
    "            B, T = idx.shape\n",
    "            # B*T x C\n",
    "            logits, _ = self(idx, None)\n",
    "            logits = logits.view(B, T, -1)\n",
    "            # only interested in prediction from last token\n",
    "            # B x C\n",
    "            logits = logits[:, -1, :].squeeze()\n",
    "            #  B x C\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            # B\n",
    "            n = torch.multinomial(probs, num_samples=1)\n",
    "            # B x T+1\n",
    "            idx = torch.cat([idx, n], dim=1)\n",
    "        return idx\n",
    "        \n",
    "m = BigramLanguageModel(len(chars))\n",
    "logits, loss = m(x, y)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resent mpaldiver pras'seve k's:\n",
      "faknd t to'le.\n",
      "Tesaip t y mivenul\n",
      "Tovecyonet sofond mis fongu rr fo hoou but\n"
     ]
    }
   ],
   "source": [
    "# generate examples\n",
    "sample_x = torch.zeros(1, 1, dtype=torch.long)\n",
    "preds = m.generate(x, 100)\n",
    "print([decode(preds[0].tolist())][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(batch_size, block_size, eval_iters=100):\n",
    "    out = {}\n",
    "    m.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            x, y = get_batch(split, batch_size, block_size)\n",
    "            logits, loss = m(x, y)\n",
    "            losses[k] = loss\n",
    "        out[f'{split}_loss'] = losses.mean().item()\n",
    "    m.train()\n",
    "    return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "def train(model, optimizer, batch_size, block_size, num_steps, eval_iters=100):\n",
    "    for step in range(num_steps):\n",
    "        x, y = get_batch('train', batch_size, block_size)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits, loss = model(x, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 500 == 0:\n",
    "            print(estimate_loss(batch_size, block_size, eval_iters))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words model  \n",
    "\n",
    "Using item embedding for each token, the simplest way to summarize sentence context is to average all item embeddings together. Then use this sentence embedding to predict next token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute BoW: option 1\n",
    "T = 3\n",
    "mask = torch.tril(torch.ones(T, T))\n",
    "mask = mask / mask.sum(-1, keepdim=True)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute BoW: option 2\n",
    "# This is better because softmax allows us to turn arbitrary values into probabilities\n",
    "T = 3\n",
    "mask = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros(T, T)\n",
    "wei = wei.masked_fill(mask == 0, float('-inf'))\n",
    "wei = torch.nn.functional.softmax(wei, dim=-1)\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagOfWordsLanguageModel(torch.nn.Module):\n",
    "    def __init__(self, block_size, vocab_size, emb_size):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.token_emb = torch.nn.Embedding(vocab_size, emb_size)\n",
    "        self.position_emb = torch.nn.Embedding(block_size, emb_size)\n",
    "        self.out = torch.nn.Linear(emb_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, target):\n",
    "        # idx: B x block_size\n",
    "        B, T = idx.shape\n",
    "        # B x block_size x emb_size\n",
    "        # each term in the sequence is mapped to an embedding\n",
    "        token_emb = self.token_emb(idx)  \n",
    "        # position_idx: block_size\n",
    "        position_idx = torch.arange(T)\n",
    "        # position_emb: block_size x emb_size\n",
    "        position_emb = self.position_emb(position_idx)\n",
    "        # B x block_size x emb_size\n",
    "        x = token_emb + position_emb\n",
    "        # mask: B x block_size x block_size\n",
    "        mask = torch.tril(torch.ones(T, T))\n",
    "        wei = torch.zeros(T, T)\n",
    "        wei = wei.masked_fill(mask == 0, float('-inf'))\n",
    "        # wei: B x T x T\n",
    "        wei = torch.nn.functional.softmax(wei, dim=-1)\n",
    "        # B x block_size x emb_size \n",
    "        bow = wei @ x\n",
    "        # B x block_size x vocab_size\n",
    "        logits = self.out(bow)\n",
    "        \n",
    "        if target is not None:\n",
    "            # target: B x block_size\n",
    "            target = target.view(-1)\n",
    "            # B*block_size x vocab_size\n",
    "            logits = logits.view(B*T, -1)\n",
    "            loss = torch.nn.functional.cross_entropy(logits, target)\n",
    "            # B x block_size x vocab_size\n",
    "            logits = logits.view(B, T, -1)\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits, loss \n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx: B x T\n",
    "        for _ in range(max_new_tokens):\n",
    "            # B x T x C\n",
    "            logits, _ = self(idx[:, -self.block_size:], None)\n",
    "            # only interested in prediction from last token\n",
    "            # B x 1 x C\n",
    "            logits = logits[:, -1, :]\n",
    "            # B x 1 x C\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            # B x 1\n",
    "            n = torch.multinomial(probs, num_samples=1)\n",
    "            # B x T+1\n",
    "            idx = torch.cat([idx, n], dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = BagOfWordsLanguageModel(8, len(chars), 32)\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=0.001)\n",
    "train(\n",
    "    model=m,\n",
    "    optimizer=optimizer, \n",
    "    batch_size=16, \n",
    "    block_size=16, \n",
    "    num_steps=5000, \n",
    "    eval_iters=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "YA\n",
      "eyALt\n",
      ":B, 'w , iat\n",
      "htsia cco uChvaooeri ldfeoten u p  aanikfDig,ot-L \n",
      "N ho.ien.a\n",
      "\n",
      "si lAmhhSe ,lte\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(1, 1, dtype=torch.long)\n",
    "out = m.generate(x, 100)\n",
    "print(decode(out[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self attention model\n",
    "\n",
    "Each word in the context can now attend to each other using dedicated linear layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale attention down, preserve unit variance, make softmax not saturate\n",
    "k = torch.randn(8, 10, 16)\n",
    "q = torch.randn(8, 10, 16)\n",
    "att = k @ q.transpose(-1, -2)\n",
    "att_probs = torch.nn.functional.softmax(att, dim=-1)\n",
    "norm_att = att / 16 ** 0.5\n",
    "norm_att_probs = torch.nn.functional.softmax(norm_att, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9328), tensor(0.9549), tensor(13.5740), tensor(0.8484))"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var(), q.var(), att.var(), norm_att.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2.0836e-04, 6.9544e-04, 6.3730e-04, 1.0454e-01, 1.1655e-04, 8.3147e-01,\n",
       "         3.9159e-02, 2.3124e-02, 1.9388e-05, 3.2084e-05]),\n",
       " tensor([0.0395, 0.0533, 0.0522, 0.1867, 0.0341, 0.3136, 0.1461, 0.1281, 0.0218,\n",
       "         0.0247]))"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_probs[0, 0], norm_att_probs[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(torch.nn.Module):\n",
    "    def __init__(self, emb_size, head_size):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.k = torch.nn.Linear(emb_size, head_size)\n",
    "        self.q = torch.nn.Linear(emb_size, head_size)\n",
    "        self.v = torch.nn.Linear(emb_size, head_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: B x T x E\n",
    "        B, T, E = x.shape\n",
    "        # B x T x L\n",
    "        k = self.k(x)\n",
    "        # B x T x L\n",
    "        q = self.q(x)\n",
    "        # B x T x L\n",
    "        v = self.v(x)\n",
    "        # B x T x T\n",
    "        att = k @ q.transpose(-2, -1)\n",
    "        att = att / (k.shape[-1] ** 0.5)\n",
    "        # B x T x T\n",
    "        mask = torch.tril(torch.ones(T, T))\n",
    "        # B x T x T\n",
    "        masked_att = att.masked_fill(mask == 0, float('-inf'))\n",
    "        # B x T x T\n",
    "        masked_att = torch.nn.functional.softmax(masked_att, dim=-1)\n",
    "        # B x T x L\n",
    "        att_bow = masked_att @ v\n",
    "        return att_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLanguageModel(torch.nn.Module):\n",
    "    def __init__(self, block_size, vocab_size, emb_size, linear_size):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.token_emb = torch.nn.Embedding(vocab_size, emb_size)\n",
    "        self.position_emb = torch.nn.Embedding(block_size, emb_size)\n",
    "        self.head = Head(emb_size, linear_size)\n",
    "        self.out = torch.nn.Linear(linear_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, target):\n",
    "        # idx: B x T\n",
    "        B, T = idx.shape\n",
    "        # B x T x E\n",
    "        token_emb = self.token_emb(idx)  \n",
    "        # position_idx: block_size\n",
    "        position_idx = torch.arange(T)\n",
    "        # position_emb: block_size x emb_size\n",
    "        position_emb = self.position_emb(position_idx)\n",
    "        # B x T x E\n",
    "        x = token_emb + position_emb\n",
    "        # B x T x L\n",
    "        att_bow = self.head(x)        \n",
    "        # B x block_size x vocab_size\n",
    "        logits = self.out(att_bow)\n",
    "        \n",
    "        if target is not None:\n",
    "            # target: B x block_size\n",
    "            target = target.view(-1)\n",
    "            # B*block_size x vocab_size\n",
    "            logits = logits.view(B*T, -1)\n",
    "            loss = torch.nn.functional.cross_entropy(logits, target)\n",
    "            # B x block_size x vocab_size\n",
    "            logits = logits.view(B, T, -1)\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits, loss \n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx: B x T\n",
    "        for _ in range(max_new_tokens):\n",
    "            # B x T x C\n",
    "            logits, _ = self(idx[:, -self.block_size:], None)\n",
    "            # only interested in prediction from last token\n",
    "            # B x 1 x C\n",
    "            logits = logits[:, -1, :]\n",
    "            # B x 1 x C\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            # B x 1\n",
    "            n = torch.multinomial(probs, num_samples=1)\n",
    "            # B x T+1\n",
    "            idx = torch.cat([idx, n], dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = AttentionLanguageModel(\n",
    "    block_size=8, \n",
    "    vocab_size=len(chars), \n",
    "    emb_size=32,\n",
    "    linear_size=32\n",
    ")\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=0.001)\n",
    "train(\n",
    "    model=m,\n",
    "    optimizer=optimizer, \n",
    "    batch_size=16, \n",
    "    block_size=16, \n",
    "    num_steps=5000, \n",
    "    eval_iters=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ald.\n",
      "My ti y'sak alsppyu\n",
      "Way oureens role to;\n",
      "USice sa yon.\n",
      "\n",
      "AMNIO:\n",
      "And wad weat rs thes ree hors as\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(1, 1, dtype=torch.long)\n",
    "out = m.generate(x, 100)\n",
    "print(decode(out[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-head attention model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add multi-head to expand the model capacity. \n",
    "\n",
    "Add feed forward layer to further reason based on weighted bag of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, emb_size, head_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.heads = torch.nn.ModuleList([\n",
    "            Head(emb_size, head_size) for _ in range(num_heads)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: B x T x E\n",
    "        B, T, E = x.shape\n",
    "        # B x T x n x H\n",
    "        att_bows = torch.stack([head(x) for head in self.heads], dim=2)\n",
    "        # B x T x n*H\n",
    "        att_bows = att_bows.view(B, T, -1)\n",
    "        return att_bows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(in_size, out_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: B x T x in\n",
    "        # B x T x out\n",
    "        x = torch.nn.functional.relu(self.linear(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLanguageModel(torch.nn.Module):\n",
    "    def __init__(self, block_size, vocab_size, emb_size, head_size, num_heads, hidden_size):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.token_emb = torch.nn.Embedding(vocab_size, emb_size)\n",
    "        self.position_emb = torch.nn.Embedding(block_size, emb_size)\n",
    "        self.multihead = MultiHeadAttention(emb_size, head_size, num_heads)\n",
    "        self.feedforward = FeedForward(head_size*num_heads, hidden_size)\n",
    "        self.out = torch.nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, target):\n",
    "        # idx: B x T\n",
    "        B, T = idx.shape\n",
    "        # B x T x E\n",
    "        token_emb = self.token_emb(idx)  \n",
    "        # position_idx: block_size\n",
    "        position_idx = torch.arange(T)\n",
    "        # position_emb: block_size x emb_size\n",
    "        position_emb = self.position_emb(position_idx)\n",
    "        # B x T x E\n",
    "        x = token_emb + position_emb\n",
    "        # B x T x H*n\n",
    "        att_bow = self.multihead(x)     \n",
    "        # B x T x hidden\n",
    "        x = self.feedforward(att_bow)   \n",
    "        # B x T x C\n",
    "        logits = self.out(x)\n",
    "        \n",
    "        if target is not None:\n",
    "            # target: B x block_size\n",
    "            target = target.view(-1)\n",
    "            # B*block_size x vocab_size\n",
    "            logits = logits.view(B*T, -1)\n",
    "            loss = torch.nn.functional.cross_entropy(logits, target)\n",
    "            # B x block_size x vocab_size\n",
    "            logits = logits.view(B, T, -1)\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits, loss \n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx: B x T\n",
    "        for _ in range(max_new_tokens):\n",
    "            # B x T x C\n",
    "            logits, _ = self(idx[:, -self.block_size:], None)\n",
    "            # only interested in prediction from last token\n",
    "            # B x 1 x C\n",
    "            logits = logits[:, -1, :]\n",
    "            # B x 1 x C\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            # B x 1\n",
    "            n = torch.multinomial(probs, num_samples=1)\n",
    "            # B x T+1\n",
    "            idx = torch.cat([idx, n], dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MultiHeadAttentionLanguageModel(\n",
    "    block_size=8, \n",
    "    vocab_size=len(chars), \n",
    "    emb_size=32,\n",
    "    head_size=32,\n",
    "    num_heads=4,\n",
    "    hidden_size=32,\n",
    ")\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=0.001)\n",
    "train(\n",
    "    model=m,\n",
    "    optimizer=optimizer, \n",
    "    batch_size=16, \n",
    "    block_size=16, \n",
    "    num_steps=5000, \n",
    "    eval_iters=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "COMIFFRe, capiis\n",
      "Whut hif kied!\n",
      "Wing: I chat I I grotheir; id dear grong of wor wer mires, thempeare\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(1, 1, dtype=torch.long)\n",
    "out = m.generate(x, 100)\n",
    "print(decode(out[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi layered attention model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(torch.nn.Module):\n",
    "    def __init__(self, in_size, head_size, num_heads, out_size):\n",
    "        super().__init__()\n",
    "        self.multihead = MultiHeadAttention(in_size, head_size, num_heads)\n",
    "        self.feedforward = FeedForward(head_size*num_heads, out_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: B x T x in\n",
    "        # B x T x n*H\n",
    "        att_bow = self.multihead(x)\n",
    "        # B x T x out\n",
    "        out = self.feedforward(att_bow)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiBlockAttentionModel(torch.nn.Module):\n",
    "    def __init__(self, block_size, vocab_size, head_size, num_heads, hidden_size, num_blocks):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.token_emb = torch.nn.Embedding(vocab_size, hidden_size)\n",
    "        self.position_emb = torch.nn.Embedding(block_size, hidden_size)\n",
    "        self.blocks = torch.nn.ModuleList([\n",
    "            Block(hidden_size, head_size, num_heads, hidden_size) for _ in range(num_blocks)\n",
    "        ])\n",
    "        self.out = torch.nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, target):\n",
    "        # idx: B x T\n",
    "        B, T = idx.shape\n",
    "        # B x T x E (hidden_size)\n",
    "        token_emb = self.token_emb(idx)  \n",
    "        # position_idx: block_size\n",
    "        position_idx = torch.arange(T)\n",
    "        # position_emb: block_size x emb_size\n",
    "        position_emb = self.position_emb(position_idx)\n",
    "        # B x T x E\n",
    "        x = token_emb + position_emb\n",
    "        # B x T x E\n",
    "        for block in self.blocks:\n",
    "            x = block(x)   \n",
    "        # B x T x C\n",
    "        logits = self.out(x)\n",
    "        \n",
    "        if target is not None:\n",
    "            # target: B x block_size\n",
    "            target = target.view(-1)\n",
    "            # B*block_size x vocab_size\n",
    "            logits = logits.view(B*T, -1)\n",
    "            loss = torch.nn.functional.cross_entropy(logits, target)\n",
    "            # B x block_size x vocab_size\n",
    "            logits = logits.view(B, T, -1)\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits, loss \n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx: B x T\n",
    "        for _ in range(max_new_tokens):\n",
    "            # B x T x C\n",
    "            logits, _ = self(idx[:, -self.block_size:], None)\n",
    "            # only interested in prediction from last token\n",
    "            # B x 1 x C\n",
    "            logits = logits[:, -1, :]\n",
    "            # B x 1 x C\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            # B x 1\n",
    "            n = torch.multinomial(probs, num_samples=1)\n",
    "            # B x T+1\n",
    "            idx = torch.cat([idx, n], dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MultiBlockAttentionModel(\n",
    "    block_size=8, \n",
    "    vocab_size=len(chars), \n",
    "    head_size=32,\n",
    "    num_heads=4,\n",
    "    hidden_size=32,\n",
    "    num_blocks=4,\n",
    ")\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=0.001)\n",
    "train(\n",
    "    model=m,\n",
    "    optimizer=optimizer, \n",
    "    batch_size=16, \n",
    "    block_size=16, \n",
    "    num_steps=5000, \n",
    "    eval_iters=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Can my'; deavy nowns\n",
      "byou,\n",
      "A comy\n",
      "A and soalinde mis lend esest.\n",
      "\n",
      "FRICCES:\n",
      "Onof a me punf theentets.\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(1, 1, dtype=torch.long)\n",
    "out = m.generate(x, 100)\n",
    "print(decode(out[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual block\n",
    "\n",
    "Learn residual to help with deep layered neural network. Validation loss should drop a lot faster comparing to non-residual blocks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, in_size, head_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.heads = torch.nn.ModuleList([\n",
    "            Head(in_size, head_size) for _ in range(num_heads)\n",
    "        ])\n",
    "        # added a new projection layer\n",
    "        self.proj = torch.nn.Linear(head_size*num_heads, in_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: B x T x E\n",
    "        B, T, E = x.shape\n",
    "        # B x T x n x H\n",
    "        att_bows = torch.stack([head(x) for head in self.heads], dim=2)\n",
    "        # B x T x n*H\n",
    "        att_bows = att_bows.view(B, T, -1)\n",
    "        # B x T x E\n",
    "        out = self.proj(att_bows)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Module):\n",
    "    def __init__(self, in_size):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_size, in_size * 4),\n",
    "            torch.nn.ReLU(),\n",
    "            # added one more linear layer \n",
    "            torch.nn.Linear(in_size * 4, in_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(torch.nn.Module):\n",
    "    def __init__(self, in_size, head_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.multihead = MultiHeadAttention(in_size, head_size, num_heads)\n",
    "        self.feedforward = FeedForward(in_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: B x T x in\n",
    "        # B x T x in_size\n",
    "        x = x + self.multihead(x)\n",
    "        # B x T x in_size\n",
    "        x = x + self.feedforward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiResidualBlockAttentionModel(torch.nn.Module):\n",
    "    def __init__(self, block_size, vocab_size, head_size, num_heads, hidden_size, num_blocks):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.token_emb = torch.nn.Embedding(vocab_size, hidden_size)\n",
    "        self.position_emb = torch.nn.Embedding(block_size, hidden_size)\n",
    "        self.blocks = torch.nn.ModuleList([\n",
    "            ResidualBlock(hidden_size, head_size, num_heads) for _ in range(num_blocks)\n",
    "        ])\n",
    "        self.out = torch.nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, target):\n",
    "        # idx: B x T\n",
    "        B, T = idx.shape\n",
    "        # B x T x E (hidden_size)\n",
    "        token_emb = self.token_emb(idx)  \n",
    "        # position_idx: block_size\n",
    "        position_idx = torch.arange(T)\n",
    "        # position_emb: block_size x emb_size\n",
    "        position_emb = self.position_emb(position_idx)\n",
    "        # B x T x E\n",
    "        x = token_emb + position_emb\n",
    "        # B x T x E\n",
    "        for block in self.blocks:\n",
    "            x = block(x)   \n",
    "        # B x T x C\n",
    "        logits = self.out(x)\n",
    "        \n",
    "        if target is not None:\n",
    "            # target: B x block_size\n",
    "            target = target.view(-1)\n",
    "            # B*block_size x vocab_size\n",
    "            logits = logits.view(B*T, -1)\n",
    "            loss = torch.nn.functional.cross_entropy(logits, target)\n",
    "            # B x block_size x vocab_size\n",
    "            logits = logits.view(B, T, -1)\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits, loss \n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx: B x T\n",
    "        for _ in range(max_new_tokens):\n",
    "            # B x T x C\n",
    "            logits, _ = self(idx[:, -self.block_size:], None)\n",
    "            # only interested in prediction from last token\n",
    "            # B x 1 x C\n",
    "            logits = logits[:, -1, :]\n",
    "            # B x 1 x C\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            # B x 1\n",
    "            n = torch.multinomial(probs, num_samples=1)\n",
    "            # B x T+1\n",
    "            idx = torch.cat([idx, n], dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MultiResidualBlockAttentionModel(\n",
    "    block_size=8, \n",
    "    vocab_size=len(chars), \n",
    "    head_size=32,\n",
    "    num_heads=4,\n",
    "    hidden_size=32,\n",
    "    num_blocks=4,\n",
    ")\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=0.001)\n",
    "train(\n",
    "    model=m,\n",
    "    optimizer=optimizer, \n",
    "    batch_size=16, \n",
    "    block_size=16, \n",
    "    num_steps=5000, \n",
    "    eval_iters=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "And you therexing, encle, the conder metin ciat and will of in hadly of me'\n",
      "The coning,\n",
      "O, pitead.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(1, 1, dtype=torch.long)\n",
    "out = m.generate(x, 100)\n",
    "print(decode(out[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer norm\n",
    "\n",
    "Normalization across layers for gradient flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(torch.nn.Module):\n",
    "    def __init__(self, in_size, head_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.multihead = MultiHeadAttention(in_size, head_size, num_heads)\n",
    "        self.feedforward = FeedForward(in_size)\n",
    "        self.layernorm1 = torch.nn.LayerNorm(in_size)\n",
    "        self.layernorm2 = torch.nn.LayerNorm(in_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: B x T x in\n",
    "        # B x T x in\n",
    "        x = self.layernorm1(x)\n",
    "        # B x T x in_size\n",
    "        x = x + self.multihead(x)\n",
    "        # B x T x in_size\n",
    "        x = self.layernorm2(x)\n",
    "        # B x T x in_size\n",
    "        x = x + self.feedforward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "class  LayerNormResidualBlockAttentionModel(torch.nn.Module):\n",
    "    def __init__(self, block_size, vocab_size, head_size, num_heads, hidden_size, num_blocks):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.token_emb = torch.nn.Embedding(vocab_size, hidden_size)\n",
    "        self.position_emb = torch.nn.Embedding(block_size, hidden_size)\n",
    "        self.blocks = torch.nn.ModuleList([\n",
    "                ResidualBlock(hidden_size, head_size, num_heads) for _ in range(num_blocks)\n",
    "            ] + [\n",
    "                torch.nn.LayerNorm(hidden_size)\n",
    "            ]\n",
    "        )\n",
    "        self.out = torch.nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, target):\n",
    "        # idx: B x T\n",
    "        B, T = idx.shape\n",
    "        # B x T x E (hidden_size)\n",
    "        token_emb = self.token_emb(idx)  \n",
    "        # position_idx: block_size\n",
    "        position_idx = torch.arange(T)\n",
    "        # position_emb: block_size x emb_size\n",
    "        position_emb = self.position_emb(position_idx)\n",
    "        # B x T x E\n",
    "        x = token_emb + position_emb\n",
    "        # B x T x E\n",
    "        for block in self.blocks:\n",
    "            x = block(x)   \n",
    "        # B x T x C\n",
    "        logits = self.out(x)\n",
    "        \n",
    "        if target is not None:\n",
    "            # target: B x block_size\n",
    "            target = target.view(-1)\n",
    "            # B*block_size x vocab_size\n",
    "            logits = logits.view(B*T, -1)\n",
    "            loss = torch.nn.functional.cross_entropy(logits, target)\n",
    "            # B x block_size x vocab_size\n",
    "            logits = logits.view(B, T, -1)\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits, loss \n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx: B x T\n",
    "        for _ in range(max_new_tokens):\n",
    "            # B x T x C\n",
    "            logits, _ = self(idx[:, -self.block_size:], None)\n",
    "            # only interested in prediction from last token\n",
    "            # B x 1 x C\n",
    "            logits = logits[:, -1, :]\n",
    "            # B x 1 x C\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            # B x 1\n",
    "            n = torch.multinomial(probs, num_samples=1)\n",
    "            # B x T+1\n",
    "            idx = torch.cat([idx, n], dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MultiResidualBlockAttentionModel(\n",
    "    block_size=8, \n",
    "    vocab_size=len(chars), \n",
    "    head_size=32,\n",
    "    num_heads=4,\n",
    "    hidden_size=32,\n",
    "    num_blocks=4,\n",
    ")\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=0.001)\n",
    "train(\n",
    "    model=m,\n",
    "    optimizer=optimizer, \n",
    "    batch_size=16, \n",
    "    block_size=16, \n",
    "    num_steps=5000, \n",
    "    eval_iters=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "lood, Loly:\n",
      "And melg,\n",
      "Fore thered:\n",
      "I am essence? I waty will and you Edward tougar; manine, cicke he\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(1, 1, dtype=torch.long)\n",
    "out = m.generate(x, 100)\n",
    "print(decode(out[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout \n",
    "\n",
    "Randomly trains the sub networks, for regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Module):\n",
    "    def __init__(self, in_size, dropout):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_size, in_size * 4),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_size * 4, in_size),\n",
    "            # add dropout\n",
    "            torch.nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(torch.nn.Module):\n",
    "    def __init__(self, emb_size, head_size, dropout):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.k = torch.nn.Linear(emb_size, head_size)\n",
    "        self.q = torch.nn.Linear(emb_size, head_size)\n",
    "        self.v = torch.nn.Linear(emb_size, head_size)\n",
    "        # add dropout\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: B x T x E\n",
    "        B, T, E = x.shape\n",
    "        # B x T x L\n",
    "        k = self.k(x)\n",
    "        # B x T x L\n",
    "        q = self.q(x)\n",
    "        # B x T x L\n",
    "        v = self.v(x)\n",
    "        # B x T x T\n",
    "        att = k @ q.transpose(-2, -1)\n",
    "        att = att / (k.shape[-1] ** 0.5)\n",
    "        # B x T x T\n",
    "        mask = torch.tril(torch.ones(T, T))\n",
    "        # B x T x T\n",
    "        masked_att = att.masked_fill(mask == 0, float('-inf'))\n",
    "        # B x T x T\n",
    "        masked_att = torch.nn.functional.softmax(masked_att, dim=-1)\n",
    "        masked_att = self.dropout(masked_att)\n",
    "        # B x T x L\n",
    "        att_bow = masked_att @ v\n",
    "        return att_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, in_size, head_size, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.heads = torch.nn.ModuleList([\n",
    "            Head(in_size, head_size, dropout) for _ in range(num_heads)\n",
    "        ])\n",
    "        self.proj = torch.nn.Linear(head_size*num_heads, in_size)\n",
    "        # add dropout\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: B x T x E\n",
    "        B, T, E = x.shape\n",
    "        # B x T x n x H\n",
    "        att_bows = torch.stack([head(x) for head in self.heads], dim=2)\n",
    "        # B x T x n*H\n",
    "        att_bows = att_bows.view(B, T, -1)\n",
    "        # B x T x E\n",
    "        out = self.proj(att_bows)\n",
    "        out = self.dropout(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(torch.nn.Module):\n",
    "    def __init__(self, in_size, head_size, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.multihead = MultiHeadAttention(in_size, head_size, num_heads, dropout)\n",
    "        self.feedforward = FeedForward(in_size, dropout)\n",
    "        self.layernorm1 = torch.nn.LayerNorm(in_size)\n",
    "        self.layernorm2 = torch.nn.LayerNorm(in_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: B x T x in\n",
    "        # B x T x in\n",
    "        x = self.layernorm1(x)\n",
    "        # B x T x in_size\n",
    "        x = x + self.multihead(x)\n",
    "        # B x T x in_size\n",
    "        x = self.layernorm2(x)\n",
    "        # B x T x in_size\n",
    "        x = x + self.feedforward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "class  DropoutAttentionModel(torch.nn.Module):\n",
    "    def __init__(self, block_size, vocab_size, num_heads, hidden_size, num_blocks, dropout):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.token_emb = torch.nn.Embedding(vocab_size, hidden_size)\n",
    "        self.position_emb = torch.nn.Embedding(block_size, hidden_size)\n",
    "        self.blocks = torch.nn.ModuleList([\n",
    "                ResidualBlock(hidden_size, hidden_size // num_heads, num_heads, dropout) for _ in range(num_blocks)\n",
    "            ] + [\n",
    "                torch.nn.LayerNorm(hidden_size)\n",
    "            ]\n",
    "        )\n",
    "        self.out = torch.nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, target):\n",
    "        # idx: B x T\n",
    "        B, T = idx.shape\n",
    "        # B x T x E (hidden_size)\n",
    "        token_emb = self.token_emb(idx)  \n",
    "        # position_idx: block_size\n",
    "        position_idx = torch.arange(T)\n",
    "        # position_emb: block_size x emb_size\n",
    "        position_emb = self.position_emb(position_idx)\n",
    "        # B x T x E\n",
    "        x = token_emb + position_emb\n",
    "        # B x T x E\n",
    "        for block in self.blocks:\n",
    "            x = block(x)   \n",
    "        # B x T x C\n",
    "        logits = self.out(x)\n",
    "        \n",
    "        if target is not None:\n",
    "            # target: B x block_size\n",
    "            target = target.view(-1)\n",
    "            # B*block_size x vocab_size\n",
    "            logits = logits.view(B*T, -1)\n",
    "            loss = torch.nn.functional.cross_entropy(logits, target)\n",
    "            # B x block_size x vocab_size\n",
    "            logits = logits.view(B, T, -1)\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits, loss \n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx: B x T\n",
    "        for _ in range(max_new_tokens):\n",
    "            # B x T x C\n",
    "            logits, _ = self(idx[:, -self.block_size:], None)\n",
    "            # only interested in prediction from last token\n",
    "            # B x 1 x C\n",
    "            logits = logits[:, -1, :]\n",
    "            # B x 1 x C\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            # B x 1\n",
    "            n = torch.multinomial(probs, num_samples=1)\n",
    "            # B x T+1\n",
    "            idx = torch.cat([idx, n], dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = DropoutAttentionModel(\n",
    "    block_size=8, \n",
    "    vocab_size=len(chars), \n",
    "    head_size=32,\n",
    "    num_heads=4,\n",
    "    hidden_size=32,\n",
    "    num_blocks=4,\n",
    "    dropout=0.1,\n",
    ")\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=0.001)\n",
    "train(\n",
    "    model=m,\n",
    "    optimizer=optimizer, \n",
    "    batch_size=16, \n",
    "    block_size=16, \n",
    "    num_steps=5000, \n",
    "    eval_iters=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wall\n",
      "Selt men wash litter awwrongecey in and nain,\n",
      "Afur hed, what\n",
      "Turull's of 'tannr. I S Madmen I p\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(1, 1, dtype=torch.long)\n",
    "out = m.generate(x, 100)\n",
    "print(decode(out[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final model\n",
    "\n",
    "Push the model and see how good the model can get. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': 3.638077735900879, 'val_loss': 3.6654458045959473}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ld/qgq208k14r15r_t09pc8z2km0000gn/T/ipykernel_25471/2738260929.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0003\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m train(\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/ld/qgq208k14r15r_t09pc8z2km0000gn/T/ipykernel_25471/3197688951.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, batch_size, block_size, num_steps, eval_iters)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/ld/qgq208k14r15r_t09pc8z2km0000gn/T/ipykernel_25471/4217155922.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx, target)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;31m# B*block_size x vocab_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0;31m# B x block_size x vocab_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3029\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "block_size = 256\n",
    "m = DropoutAttentionModel(\n",
    "    block_size=block_size, \n",
    "    vocab_size=len(chars), \n",
    "    head_size=32,\n",
    "    num_heads=6,\n",
    "    hidden_size=384,\n",
    "    num_blocks=6,\n",
    "    dropout=0.2,\n",
    ")\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=0.0003)\n",
    "train(\n",
    "    model=m, \n",
    "    optimizer=optimizer, \n",
    "    batch_size=64,  \n",
    "    block_size=block_size,\n",
    "    num_steps=5000,\n",
    "    eval_iters=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RUMA'st theas aro nod ond e sheru\n",
      "GRWesea he thoul feafr, 'st:\n",
      "Sthe lof lo, ishon h!\n",
      "He wnk Romime o he al haird,\n",
      "Asovevery,\n",
      "Ay shiace nor t he O we be oouthtotrlothe ivarepr the\n",
      "Whor.\n",
      "PFr; s hatherendis\n",
      "RDWht o s;\n",
      "NRo CARICKES:\n",
      "Hoootit s:\n",
      "K:\n",
      "\n",
      "\n",
      "CLI lldo henacar y s pbrise youlf:\n",
      "IIURESARDithe:\n",
      "Ay e mpen sins wo hithowes,\n",
      "AURTeced inonoiny hererdof ar histhicary un um h IO:\n",
      "Ia gais ur, we:\n",
      "ARI ou attsout TEve on n:\n",
      "IOLARLelavayou herot I:\n",
      "\n",
      "DUSe adl y hareasu ESe! y gouthe s pe wse d, oflapurince t m had learean arinore in\n",
      "Pr qrou ano theind at:\n",
      "Ce rago hue sigrk;\n",
      "Wiceror y.\n",
      "Yo, inoouce's IA Vhaist premayotel sigu ee--ongour be, desh mie. bror's:\n",
      ":\n",
      "OYUCEORou Fothisaker,\n",
      "TUS:\n",
      "Toustroe INA buthille whantheu hblee y is,\n",
      "A pre. a I harldest COnerd gi wisees abel, wilo my?\n",
      "YIEN K:\n",
      "NEurt o,\n",
      "TARESaner, t mm? nd, he mouryoroiomietrthingomeryond atef onour 'deduin LO:\n",
      "Ayenkeghentu pank; med DO:\n",
      "LAD:\n",
      "Hetreseat yoverd f gorsith, g he'lithat cave bydeath. pelodare IV: huro;\n",
      "Ar du.\n",
      "Whandsu y, urco f \n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(1, 1, dtype=torch.long)\n",
    "out = m.generate(x, 1000)\n",
    "print(decode(out[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
